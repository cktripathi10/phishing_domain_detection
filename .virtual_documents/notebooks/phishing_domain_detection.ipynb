#import the necessary files
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns





#loading the dataset
df= pd.read_csv("dataset_full.csv")


df.head()





df.info()


df.describe()





#Check for missing values and duplicate rows in the dataset
missing_values= df.isnull().sum().sum()
duplicates= df.duplicated().sum()
missing_values, duplicates


#remove duplicate rows
df_cleaned= df.drop_duplicates()

#Confirm the removal

df_cleaned.shape[0]





#setting the aesthetic style of the target variable
sns.set(style="whitegrid")

#Distribution of the target variable
fig, ax = plt.subplots(1,2, figsize=(14,5))

sns.countplot(x='phishing', data= df_cleaned, ax=ax[0])
ax[0].set_title('Distribution of Target Variable(Phishing)', fontsize=14)
ax[0].set_xlabel('Phishing label', fontsize=12)
ax[0].set_ylabel('Count', fontsize=12)


#histogram of URL length(we need to calculate this if not directly available)
df_cleaned['url_length']= df_cleaned.apply(lambda row: len(str(row)), axis=1)
sns.histplot(df_cleaned['url_length'], bins=30, ax=ax[1], kde=True)
ax[1].set_title('Distribution of URL Length', fontsize=14)
ax[1].set_xlabel('URL Length', fontsize=12)
ax[1].set_ylabel('Frequency', fontsize=12)

plt.tight_layout()
plt.show()












# Assume naming convention reflects the feature type, grouping features by their assumed categories
url_features = [col for col in df_cleaned.columns if 'url' in col]
domain_features = [col for col in df_cleaned.columns if 'domain' in col or 'dns' in col or 'ttl' in col or 'mx' in col or 'nameservers' in col]
page_features = [col for col in df_cleaned.columns if 'page' in col or 'content_length' in col or 'script' in col]
content_features = [col for col in df_cleaned.columns if 'content' in col and 'content_length' not in col]

# Print out the feature groups for confirmation
url_features, domain_features, page_features, content_features









from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Extract features and target
X = df_cleaned.drop('phishing', axis=1)
y = df_cleaned['phishing']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing the features (normalization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Checking the shapes of the resulting sets
X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape



'''
Here, I'll choose a Random Forest Classifier, which is effective 
for this type of classification task because of its robustness 
and ability to handle unbalanced data.

'''

from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Build the pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Train the model
pipeline.fit(X_train, y_train)

# Predict and evaluate the model
predictions = pipeline.predict(X_test)
print("Accuracy:", accuracy_score(y_test, predictions))
print(classification_report(y_test, predictions))



